{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taguch1s/study/blob/main/pactical-nlp-ja/03_text_embedding_to_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuu5HXaWLWSM"
      },
      "source": [
        "@see https://github.com/oreilly-japan/practical-nlp-ja/tree/master/ch03\n",
        "\n",
        "# 1.テキストのOne-Hotエンコーディング\n",
        "\n",
        "このノートブックでは、one-hotエンコーディングを実装します。実際のプロジェクトでは、scikit-learnの実装を使うことのほうが多いでしょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3i0oA1JIO5ML"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvndsBIzLWSQ",
        "outputId": "d33e2ecd-cffb-401f-d204-2da230d3bf9d"
      },
      "source": [
        "documents = [\n",
        "    \"Dog bites man.\",\n",
        "    \"Man bites dog.\",\n",
        "    \"Dog eats meat.\",\n",
        "    \"Man eats food.\"\n",
        "]\n",
        "processed_docs = [doc.lower().replace(\".\", \"\") for doc in documents]\n",
        "processed_docs"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWJcOZLBLWSW",
        "outputId": "22d8a686-c6cb-4cf0-eb81-40b0b3a2522c"
      },
      "source": [
        "# ボキャブラリの構築\n",
        "vocab = {}\n",
        "count = 0\n",
        "for doc in processed_docs:\n",
        "    for word in doc.split():\n",
        "        if word not in vocab:\n",
        "            count = count + 1\n",
        "            vocab[word] = count\n",
        "print(vocab)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'dog': 1, 'bites': 2, 'man': 3, 'eats': 4, 'meat': 5, 'food': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pesdRwpLWSc"
      },
      "source": [
        "# 与えたテキストに対応するone-hot表現を取得\n",
        "# 単語がボキャブラリに存在する場合、その表現が返される。\n",
        "# 存在しない場合は、ゼロのリストが返される。\n",
        "def get_onehot_vector(somestring):\n",
        "    onehot_encoded = []\n",
        "    for word in somestring.split():\n",
        "        temp = [0] * len(vocab)\n",
        "        if word in vocab:\n",
        "            temp[vocab[word] - 1] = 1 # リストのインデックスは1ではなく0から始まるため-1している\n",
        "        onehot_encoded.append(temp)\n",
        "    return onehot_encoded"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2-GIjI8770K"
      },
      "source": [
        "コーパスのテキストに対して、one-hot表現を取得してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JELqSh4gLWSg",
        "outputId": "6d4664fd-28dd-4865-f988-59371b31b102"
      },
      "source": [
        "print(processed_docs[1])\n",
        "get_onehot_vector(processed_docs[1])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "man bites dog\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAPeqVFu8Fam"
      },
      "source": [
        "同じボキャブラリを使って、ランダムなテキストに対するone-hot表現を取得してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVQExJUGLWSm",
        "outputId": "e497061f-1a18-4abc-cea6-0867cc2aa536"
      },
      "source": [
        "get_onehot_vector(\"man and dog are good\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 1, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0],\n",
              " [1, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xb8azVwLWSs",
        "outputId": "e77a8bd8-a256-42a1-d98b-fbc65de95026"
      },
      "source": [
        "get_onehot_vector(\"man and man are good\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 1, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 1, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANj41SQ4L7xI"
      },
      "source": [
        "## scikit-learnを用いたOne-hotエンコーディング\n",
        "\n",
        "ここでは、scikit-learnのOneHotEncoderを用いて、one-hotエンコーディングをしてみましょう。\n",
        "\n",
        "具体的には、次のことを試します。\n",
        "\n",
        "* One-Hotエンコーディング: one-hotエンコーディングでは、ボキャブラリの各単語wに、1から|V|の間の一意の整数IDとしてwidを与える。各単語は、0と1からなるV次元の二値ベクトルで表現される。\n",
        "\n",
        "* ラベルエンコーディング: ラベルエンコーディングでは、コーパスの各単語wを0からn-1までの数値に変換する。ここでnはコーパスに含まれる一意な単語数を意味する。\n",
        "\n",
        "参考\n",
        "\n",
        "- [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n",
        "- [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) respectively.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAPkk-fZLh4W"
      },
      "source": [
        "S1 = 'dog bites man'\n",
        "S2 = 'man bites dog'\n",
        "S3 = 'dog eats meat'\n",
        "S4 = 'man eats food'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYCRHl5SLWSy",
        "outputId": "96790402-d833-47cf-8028-d8306c93b70c"
      },
      "source": [
        "from itertools import chain\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "data = [S1.split(), S2.split(), S3.split(), S4.split()]\n",
        "values = list(chain.from_iterable(data))\n",
        "print(f\"The data: {values}\")\n",
        "\n",
        "# Label Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(values)\n",
        "print(f\"Label Encoded: {integer_encoded}\")\n",
        "\n",
        "# One-Hot Encoding\n",
        "onehot_encoder = OneHotEncoder()\n",
        "onehot_encoded = onehot_encoder.fit_transform(data).toarray()\n",
        "print(f\"Onehot Encoded Matrix:\\n{onehot_encoded}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The data: ['dog', 'bites', 'man', 'man', 'bites', 'dog', 'dog', 'eats', 'meat', 'man', 'eats', 'food']\n",
            "Label Encoded: [1 0 4 4 0 1 1 2 5 4 2 3]\n",
            "Onehot Encoded Matrix:\n",
            "[[1. 0. 1. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 1. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 1. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 1. 0. 1. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5T2dRue-f0I"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Bag of Words"
      ],
      "metadata": {
        "id": "3uGa9682Ngo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"Dog bites man.\",\n",
        "    \"Man bites dog.\",\n",
        "    \"Dog eats meat.\",\n",
        "    \"Man eats food.\"\n",
        "]\n",
        "processed_docs = [doc.lower().replace(\".\", \"\") for doc in documents]\n",
        "processed_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1yaJv7YNu50",
        "outputId": "b165f752-da5c-4c09-92ea-dcf3f63bd7fb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "scikit-learnのCountVectorizerを使うと、bag of wordsを簡単に作成できる"
      ],
      "metadata": {
        "id": "pGWZNyT4N1N5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# 文書のリストを確認\n",
        "print(\"Our corpus: \", processed_docs)\n",
        "\n",
        "count_vect = CountVectorizer()\n",
        "# BoW表現の構築\n",
        "bow_rep = count_vect.fit_transform(processed_docs)\n",
        "\n",
        "# ボキャブラリの確認\n",
        "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
        "\n",
        "# 最初の2つの文書に対するBoWを確認\n",
        "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
        "print(\"BoW representation for 'man bites dog': \", bow_rep[1].toarray())\n",
        "\n",
        "# 新しいテキストに対するBoWを取得\n",
        "temp = count_vect.transform([\"dog and dog are friends\"])\n",
        "print(\"BoW representation for 'dog and dog are friends': \", temp.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35ZOFtCgNlFu",
        "outputId": "2a841324-e492-4915-e551-a8c550a7cd3f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our corpus:  ['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']\n",
            "Our vocabulary:  {'dog': 1, 'bites': 0, 'man': 4, 'eats': 2, 'meat': 5, 'food': 3}\n",
            "BoW representation for 'dog bites man':  [[1 1 0 0 1 0]]\n",
            "BoW representation for 'man bites dog':  [[1 1 0 0 1 0]]\n",
            "BoW representation for 'dog and dog are friends':  [[0 2 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "文章中の登場回数を考慮しない場合は以下のように `CountVectorizer`を`binary=True`で初期化する"
      ],
      "metadata": {
        "id": "v8xK27IzOZNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_vect = CountVectorizer(binary=True)\n",
        "bow_rep_bin = count_vect.fit_transform(processed_docs)\n",
        "temp = count_vect.transform([\"dog and dog are friends\"])\n",
        "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWQ5zoJ4NwW9",
        "outputId": "e59b26f7-7cbb-4199-d251-f10cae160906"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bow representation for 'dog and dog are friends': [[0 1 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Bag of N-grams"
      ],
      "metadata": {
        "id": "p0_SsUrtO4Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# init\n",
        "documents = [\n",
        "    \"Dog bites man.\",\n",
        "    \"Man bites dog.\",\n",
        "    \"Dog eats meat.\",\n",
        "    \"Man eats food.\"\n",
        "]\n",
        "processed_docs = [doc.lower().replace(\".\", \"\") for doc in documents]\n",
        "processed_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOKiwM_XPdYq",
        "outputId": "48db7723-f7f2-45bb-f886-30af43409179"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CountVectorizerを用いてnグラム（n=1, 2, 3）のベクトル化\n",
        "count_vect = CountVectorizer(ngram_range=(1, 3))\n",
        "\n",
        "# BoN表現の構築\n",
        "bow_rep = count_vect.fit_transform(processed_docs)\n",
        "\n",
        "# 語彙の確認\n",
        "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
        "\n",
        "# 新しいテキストに対するBoNの確認\n",
        "temp = count_vect.transform([\"dog and dog are friends\"])\n",
        "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKkwun1pOftI",
        "outputId": "8e62492e-f1d4-4704-88df-7cf642c74166"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our vocabulary:  {'dog': 3, 'bites': 0, 'man': 12, 'dog bites': 4, 'bites man': 2, 'dog bites man': 5, 'man bites': 13, 'bites dog': 1, 'man bites dog': 14, 'eats': 8, 'meat': 17, 'dog eats': 6, 'eats meat': 10, 'dog eats meat': 7, 'food': 11, 'man eats': 15, 'eats food': 9, 'man eats food': 16}\n",
            "Bow representation for 'dog and dog are friends': [[0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.TF-IDF\n",
        "テキスト内の単語に重要性という概念を初めて導入した手法"
      ],
      "metadata": {
        "id": "Am60nX8aP9gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"Dog bites man.\",\n",
        "    \"Man bites dog.\",\n",
        "    \"Dog eats meat.\",\n",
        "    \"Man eats food.\"\n",
        "]\n",
        "processed_docs = [doc.lower().replace(\".\", \"\") for doc in documents]\n",
        "processed_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZhfqs7aP98v",
        "outputId": "49b20261-85d2-4faf-93b1-4cf4dad7e825"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "bow_rep_tfidf = tfidf.fit_transform(processed_docs)\n",
        "\n",
        "# ボキャブラリの全単語に対するIDF\n",
        "print(\"IDF for all words in the vocabulary\",tfidf.idf_)\n",
        "print(\"-\" * 10)\n",
        "\n",
        "# ボキャブラリの全単語\n",
        "print(\"All words in the vocabulary\",tfidf.get_feature_names_out())\n",
        "print(\"-\"*10)\n",
        "\n",
        "# 全文書に対するTFIDF\n",
        "print(\"TFIDF representation for all documents in our corpus\\n\",bow_rep_tfidf.toarray())\n",
        "print(\"-\"*10)\n",
        "\n",
        "temp = tfidf.transform([\"dog and man are friends\"])\n",
        "print(\"Tfidf representation for 'dog and man are friends':\\n\", temp.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHusvedhQKD6",
        "outputId": "5126985c-9a42-4f4a-8cbf-e47e3cff8cd9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IDF for all words in the vocabulary [1.51082562 1.22314355 1.51082562 1.91629073 1.22314355 1.91629073]\n",
            "----------\n",
            "All words in the vocabulary ['bites' 'dog' 'eats' 'food' 'man' 'meat']\n",
            "----------\n",
            "TFIDF representation for all documents in our corpus\n",
            " [[0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
            " [0.65782931 0.53256952 0.         0.         0.53256952 0.        ]\n",
            " [0.         0.44809973 0.55349232 0.         0.         0.70203482]\n",
            " [0.         0.         0.55349232 0.70203482 0.44809973 0.        ]]\n",
            "----------\n",
            "Tfidf representation for 'dog and man are friends':\n",
            " [[0.         0.70710678 0.         0.         0.70710678 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "scikit-learn >= 1.0.x から`get_feature_names()` は `get_feature_names_out()`に変更\n",
        "\n",
        "https://stackoverflow.com/questions/70215049/attributeerror-tfidfvectorizer-object-has-no-attribute-get-feature-names-out"
      ],
      "metadata": {
        "id": "Fo35ako3QfGE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yZqQdD-yQwwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.学習済みモデルを用いた分散表現"
      ],
      "metadata": {
        "id": "8i039MAdRvB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gensim==4.1.2 spacy==3.1.2"
      ],
      "metadata": {
        "id": "HqCiTLr5R4lw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import psutil # 実行中のプロセスやシステムのリソース活用について調べるため\n",
        "import time\n",
        "import warnings # 生成される様々な警告を無視するため\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "process = psutil.Process(os.getpid())\n",
        "mem = psutil.virtual_memory()"
      ],
      "metadata": {
        "id": "w73i9nbWR7S1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 事前学習済み埋め込みのダウンロード"
      ],
      "metadata": {
        "id": "GnMMlbVVSDS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P /tmp/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "metadata": {
        "id": "TOAO3ANZR_vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "path = '/tmp/input/GoogleNews-vectors-negative300.bin.gz'\n",
        "\n",
        "pre = process.memory_info().rss\n",
        "print(f\"モデル読み込み前のメモリ使用量（GB）: {pre/(10**9):.2f}\")\n",
        "print('-' * 10)\n",
        "\n",
        "start_time = time.time() # タイマーのスタート\n",
        "ttl = mem.total          # 利用可能なメモリ量\n",
        "\n",
        "#　モデルの読み込み\n",
        "vectors = KeyedVectors.load_word2vec_format(path, binary=True)\n",
        "print(f\"読み込みにかかった時間（秒）: {time.time() - start_time:.2f}\")\n",
        "print('-' * 10)\n",
        "\n",
        "print('モデルの読み込み完了')\n",
        "print('-' * 10)\n",
        "\n",
        "post = process.memory_info().rss\n",
        "print(f\"モデル読み込み後のメモリ使用量（GB）: {post/(10**9):.2f}\")\n",
        "print('-' * 10)\n",
        "\n",
        "print(f\"メモリ使用量の増加率: {post/pre*100:.2f}%\")\n",
        "print('-' * 10)\n",
        "\n",
        "print(f\"ボキャブラリ数: {len(vectors.key_to_index)}\")"
      ],
      "metadata": {
        "id": "GdbbE6MKSIln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 類似した単語の検索\n",
        "vectors.most_similar('beautiful')"
      ],
      "metadata": {
        "id": "d_8Lmvr_SLFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectors.most_similar('toronto')"
      ],
      "metadata": {
        "id": "r1uj_mTzVasG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ベクトル表現の確認\n",
        "vectors['beautiful']"
      ],
      "metadata": {
        "id": "Xtt9ovdNVcKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.gensimを用いた単語埋め込みの学習"
      ],
      "metadata": {
        "id": "2cX05YcXVi7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import warnings\n",
        "\n",
        "from gensim.models import FastText, Word2Vec, KeyedVectors\n",
        "from gensim.corpora.wikicorpus import WikiCorpus\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "t4MGEZpHVneR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gensimのword2vecでは、リストのリスト形式での学習データが必要です。\n",
        "# 各リストが1つの文書を表し、文書はトークンのリストで表されます。\n",
        "corpus = [\n",
        "    ['dog', 'bites', 'man'],\n",
        "    ['man', 'bites', 'dog'],\n",
        "    ['dog', 'eats', 'meat'],\n",
        "    ['man', 'eats', 'food']\n",
        "]"
      ],
      "metadata": {
        "id": "eD9HJNewVpSw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kYCIqP_cVsB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Continuous Bag of Words (CBOW)"
      ],
      "metadata": {
        "id": "9F5bE4w0Vtkl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CBOWの主なタスクは、文脈語を与えたときに、その中心語を正しく予測できる言語モデルを構築することです。 gensimのWord2Vecクラスに学習データとハイパーパラメータを指定することで学習できます。 学習アルゴリズムは、sgパラメータで指定します。CBOWを使うなら0、Skip-gramを使うなら1を指定します。その他のパラメータについては、以下のドキュメントを参照してください。\n",
        "\n",
        "[gensim.models.word2vec.Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec)"
      ],
      "metadata": {
        "id": "uFT8wEGhV0Qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_cbow.wv.index_to_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQ75BY_JXfpK",
        "outputId": "9843a1c3-f5e9-45ae-8c2e-baaf9fe66efb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['man', 'dog', 'eats', 'bites', 'food', 'meat']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの学習\n",
        "model_cbow = Word2Vec(corpus, min_count=1, sg=0)\n",
        "\n",
        "# モデルのサマリー\n",
        "print(model_cbow)\n",
        "\n",
        "# ボキャブラリの表示\n",
        "words = list(model_cbow.wv.index_to_key)\n",
        "print(words)\n",
        "\n",
        "# 単語ベクトルの取得\n",
        "print(model_cbow.wv['dog'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEtPACPJV02O",
        "outputId": "2ff47e09-8be8-49b7-b9f3-53d6bfc3289e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec(vocab=6, vector_size=100, alpha=0.025)\n",
            "['man', 'dog', 'eats', 'bites', 'food', 'meat']\n",
            "[-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419385e-03\n",
            "  7.4669183e-03 -6.1676754e-03  1.1056137e-03  6.0472824e-03\n",
            " -2.8400505e-03 -6.1735227e-03 -4.1022300e-04 -8.3689485e-03\n",
            " -5.6000124e-03  7.1045388e-03  3.3525396e-03  7.2256695e-03\n",
            "  6.8002474e-03  7.5307419e-03 -3.7891543e-03 -5.6180597e-04\n",
            "  2.3483764e-03 -4.5190323e-03  8.3887316e-03 -9.8581640e-03\n",
            "  6.7646410e-03  2.9144168e-03 -4.9328315e-03  4.3981876e-03\n",
            " -1.7395747e-03  6.7113843e-03  9.9648498e-03 -4.3624435e-03\n",
            " -5.9933780e-04 -5.6956373e-03  3.8508223e-03  2.7866268e-03\n",
            "  6.8910765e-03  6.1010956e-03  9.5384968e-03  9.2734173e-03\n",
            "  7.8980681e-03 -6.9895042e-03 -9.1558648e-03 -3.5575271e-04\n",
            " -3.0998408e-03  7.8943167e-03  5.9385742e-03 -1.5456629e-03\n",
            "  1.5109634e-03  1.7900408e-03  7.8175711e-03 -9.5101865e-03\n",
            " -2.0553112e-04  3.4691966e-03 -9.3897223e-04  8.3817719e-03\n",
            "  9.0107834e-03  6.5365066e-03 -7.1162102e-04  7.7104042e-03\n",
            " -8.5343346e-03  3.2071066e-03 -4.6379971e-03 -5.0889552e-03\n",
            "  3.5896183e-03  5.3703394e-03  7.7695143e-03 -5.7665063e-03\n",
            "  7.4333609e-03  6.6254963e-03 -3.7098003e-03 -8.7456414e-03\n",
            "  5.4374672e-03  6.5097557e-03 -7.8755023e-04 -6.7098560e-03\n",
            " -7.0859254e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
            " -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
            "  1.2085581e-03 -2.0748770e-03  2.4403334e-05 -9.8835090e-03\n",
            "  2.6920044e-03 -4.7501065e-03  1.0876465e-03 -1.5762246e-03\n",
            "  2.1966731e-03 -7.8815762e-03 -2.7171839e-03  2.6631986e-03\n",
            "  5.3466819e-03 -2.3915148e-03 -9.5100943e-03  4.5058788e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 類似度の計算\n",
        "print(\"Similarity between eats and bites:\", model_cbow.wv.similarity('eats', 'bites'))\n",
        "print(\"Similarity between eats and man:\", model_cbow.wv.similarity('eats', 'man'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEIjZipAWBAA",
        "outputId": "1d0ae946-4709-45ed-b96a-c5b75d123545"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between eats and bites: -0.0134970825\n",
            "Similarity between eats and man: -0.05235437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_cbow.wv.most_similar('meat')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNaVA4--YsR2",
        "outputId": "d2638886-9131-407d-fe9c-2b7f56fee56c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('food', 0.13887985050678253),\n",
              " ('bites', 0.13149003684520721),\n",
              " ('eats', 0.06422408670186996),\n",
              " ('dog', 0.009391166269779205),\n",
              " ('man', -0.05987630784511566)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの保存\n",
        "model_cbow.save('model_cbow.bin')\n",
        "\n",
        "# モデルの読み込み\n",
        "new_model_cbow = Word2Vec.load('model_cbow.bin')\n",
        "print(new_model_cbow)"
      ],
      "metadata": {
        "id": "EcjowBlTZXNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CBOWの方が学習時間はかかるが，精度で勝るということらしい"
      ],
      "metadata": {
        "id": "k1SWRcwVb63B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wikiコーパスを用いた埋め込みの学習\n",
        "\n",
        "ここまでで、小さなデータを使ってモデルを学習する方法は紹介しました。 次は、より大きなデータを使ってモデルを学習してみましょう。 ただ、そのすべてを利用すると全体で数GBを超えるので、今回はその一部だけを使って、word2vecとfastTextの埋め込みを学習します。コーパスは以下のページからダウンロードできます。\n",
        "\n",
        "https://dumps.wikimedia.org/jawiki/"
      ],
      "metadata": {
        "id": "6nKex5Gmar7w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データのダウンロード\n"
      ],
      "metadata": {
        "id": "K5vANuZgdD2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data/ja/\n",
        "!wget -P data/ja/ https://dumps.wikimedia.org/jawiki/20240501/jawiki-20240501-pages-articles6.xml-p4307948p4926292.bz2"
      ],
      "metadata": {
        "id": "xS9CnKHNas9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## パッケージのインストール\n",
        "\n",
        "日本語の場合、学習前に単語分割が必要なので、形態素解析器「MeCab」のラッパーである「fugashi」をインストールします。"
      ],
      "metadata": {
        "id": "syOF2sC7dNi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fugashi[unidic-lite]"
      ],
      "metadata": {
        "id": "q9IE3bWhdO1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 前処理\n",
        "\n",
        "インストールを終えたら、単語分割用の関数を定義します。 学習データの準備にgensimのWikiCorpusを利用するため、その内部で必要とされる形式の関数を定義しています。パラメータは以下の通りです。\n",
        "\n",
        "* content (str): Wikiのマークアップを除去した文字列\n",
        "* token_min_len (int): 最小のトークン長\n",
        "* token_max_len (int): 最大のトークン長\n",
        "* lower (bool): 文字列を小文字化するか否か"
      ],
      "metadata": {
        "id": "HFfweL6ddTLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fugashi import Tagger\n",
        "from gensim.corpora.wikicorpus import TOKEN_MAX_LEN, WikiCorpus\n",
        "from gensim import utils\n",
        "fugger = Tagger('-Owakati')\n",
        "TOKEN_MIN_LEN = 1\n",
        "\n",
        "def tokenize(\n",
        "    content,\n",
        "    token_min_len=TOKEN_MIN_LEN,\n",
        "    token_max_len=TOKEN_MAX_LEN,\n",
        "    lower=True\n",
        "  ):\n",
        "    return [\n",
        "        utils.to_unicode(token.surface) for token in fugger(content.lower() if lower else content)\n",
        "    ]"
      ],
      "metadata": {
        "id": "u1HyQHMedQN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "PxyOp6vmfarw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7MMLnfmNfbkX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}